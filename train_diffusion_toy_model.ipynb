{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. We import basic packages and define fundamental utensils. The U-Net model is directly imported and you do not have to modify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "from diff_utils import Unet, ExponentialMovingAverage\n",
    "from tqdm import tqdm\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms \n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def diffusion_mnist_dataloader(batch_size,image_size=28,num_workers=4):\n",
    "    \n",
    "    preprocess=transforms.Compose([transforms.Resize(image_size),\\\n",
    "                                    transforms.ToTensor(),\\\n",
    "                                    transforms.Normalize([0.5],[0.5])]) #[0,1] to [-1,1]\n",
    "\n",
    "    train_dataset=MNIST(root=\"./mnist_data\",\\\n",
    "                        train=True,\\\n",
    "                        download=True,\\\n",
    "                        transform=preprocess\n",
    "                        )\n",
    "    test_dataset=MNIST(root=\"./mnist_data\",\\\n",
    "                        train=False,\\\n",
    "                        download=True,\\\n",
    "                        transform=preprocess\n",
    "                        )\n",
    "\n",
    "    return DataLoader(train_dataset,batch_size=batch_size,shuffle=True,num_workers=num_workers),\\\n",
    "            DataLoader(test_dataset,batch_size=batch_size,shuffle=True,num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. We define the Diffusion toy model in this block. Please fill in the forward diffusion process and reverse diffusion process. Please observe the related varieties which may provide guidance and hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionToyModelforMNIST(nn.Module):\n",
    "    def __init__(self, image_size, in_channels, time_embedding_dim=256, timesteps=1000, base_dim=32, dim_mults= [1, 2, 4, 8]):\n",
    "        super().__init__()\n",
    "        self.timesteps=timesteps\n",
    "        self.in_channels=in_channels\n",
    "        self.image_size=image_size\n",
    "\n",
    "        betas=self._cosine_variance_schedule_generator(timesteps)\n",
    "\n",
    "        alphas=1.-betas\n",
    "        alphas_cumprod=torch.cumprod(alphas, dim=-1)\n",
    "\n",
    "        # It is an optional choice to use register_buffer. Other methods are applicable as well.\n",
    "        self.register_buffer(\"betas\", betas)\n",
    "        self.register_buffer(\"alphas\", alphas)\n",
    "        self.register_buffer(\"alphas_cumprod\", alphas_cumprod)\n",
    "        self.register_buffer(\"sqrt_alphas_cumprod\", torch.sqrt(alphas_cumprod))\n",
    "        self.register_buffer(\"sqrt_one_minus_alphas_cumprod\", torch.sqrt(1.-alphas_cumprod))\n",
    "\n",
    "        self.model=Unet(timesteps,time_embedding_dim,in_channels,in_channels,base_dim,dim_mults)\n",
    "\n",
    "    def forward(self,x,noise):\n",
    "        # x:NCHW\n",
    "        t=torch.randint(0,self.timesteps,(x.shape[0],)).to(x.device)\n",
    "        x_t=self._forward_diffusion(x,t,noise)\n",
    "        pred_noise=self.model(x_t,t)\n",
    "\n",
    "        return pred_noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sampling(self,n_samples,clipped_reverse_diffusion=True,device=\"cuda\"):\n",
    "        x_t=torch.randn((n_samples,self.in_channels,self.image_size,self.image_size)).to(device)\n",
    "        for i in tqdm(range(self.timesteps-1,-1,-1),desc=\"Sampling\"):\n",
    "            noise=torch.randn_like(x_t).to(device)\n",
    "            t=torch.tensor([i for _ in range(n_samples)]).to(device)\n",
    "\n",
    "            x_t=self._reverse_diffusion(x_t,t,noise)\n",
    "\n",
    "        x_t=(x_t+1.)/2. #[-1,1] to [0,1]\n",
    "\n",
    "        return x_t\n",
    "    \n",
    "    def _cosine_variance_schedule_generator(self, timesteps, epsilon= 0.008):\n",
    "        steps=torch.linspace(0,timesteps,steps=timesteps+1,dtype=torch.float32)\n",
    "        f_t=torch.cos(((steps/timesteps+epsilon)/(1.0+epsilon))*math.pi*0.5)**2\n",
    "        betas=torch.clip(1.0-f_t[1:]/f_t[:timesteps],0.0,0.999)\n",
    "\n",
    "        return betas\n",
    "\n",
    "    def _forward_diffusion(self,x_0,t,noise):\n",
    "        assert x_0.shape==noise.shape\n",
    "        #q(x_{t}|x_{t-1})\n",
    "        #### Fill in the forward process here ####\n",
    "        \n",
    "        \n",
    "        #### Fill in the forward process here ####\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _reverse_diffusion(self,x_t,t,noise): \n",
    "        '''\n",
    "        p(x_{0}|x_{t}),q(x_{t-1}|x_{0},x_{t})->mean,std\n",
    "\n",
    "        pred_noise -> pred_x_0 (clip to [-1.0,1.0]) -> pred_mean and pred_std\n",
    "        '''\n",
    "        pred=self.model(x_t,t)\n",
    "        alpha_t=self.alphas.gather(-1,t).reshape(x_t.shape[0],1,1,1)\n",
    "        alpha_t_cumprod=self.alphas_cumprod.gather(-1,t).reshape(x_t.shape[0],1,1,1)\n",
    "        beta_t=self.betas.gather(-1,t).reshape(x_t.shape[0],1,1,1)\n",
    "        #### Fill in the backward process here ####\n",
    "        \n",
    "        \n",
    "        #### Fill in the backward process here ####\n",
    "\n",
    "        if t.min()>0:\n",
    "            #### Fill in the backward process here ####\n",
    "            \n",
    "            \n",
    "            #### Fill in the backward process here ####\n",
    "        else:\n",
    "            #### Fill in the backward process here ####\n",
    "            \n",
    "            #### Fill in the backward process here ####\n",
    "            std=0.0\n",
    "\n",
    "        return mean+std*noise \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define all of the hyperparameters. You can customize them based on your own training environment, if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 128   # If default 128 batch size is too large for your GPU, try reducing it.\n",
    "model_ema_steps = 10\n",
    "model_ema_decay = 0.995  # These two parameters are used for the Exponential Moving Average (EMA) of the model. \n",
    "log_frequency = 50  # The frequency for printing the log message during training.\n",
    "lr = 0.001  # Learning rate.\n",
    "timesteps = 1000      # Sampling steps of the diffusion DDPM process.\n",
    "\n",
    "# We do not recommand you to change below hyperparameters, maintaining a fair training comparison and unified visualization.\n",
    "epochs = 100\n",
    "n_samples = 36\n",
    "model_base_dim = 64   # The base dimension of the UNet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. This is the main training process of our toy diffusion model. Please fill in the blank to complete the loss calulation and optimization steps. We provide multiple ways for you to observe your training progress for your customization. Try and play!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"device:\",device)\n",
    "train_dataloader,test_dataloader=diffusion_mnist_dataloader(batch_size=batch_size,image_size=28)\n",
    "model=DiffusionToyModelforMNIST(timesteps=timesteps,\n",
    "            image_size=28,\n",
    "            in_channels=1,\n",
    "            base_dim=model_base_dim,\n",
    "            dim_mults=[2,4]).to(device)\n",
    "\n",
    "#torchvision ema setting\n",
    "#https://github.com/pytorch/vision/blob/main/references/classification/train.py#L317\n",
    "adjust = 1 * batch_size * model_ema_steps / epochs\n",
    "alpha = 1.0 - model_ema_decay\n",
    "alpha = min(1.0, alpha * adjust)\n",
    "model_ema = ExponentialMovingAverage(model, device=device, decay=1.0 - alpha)\n",
    "\n",
    "optimizer=AdamW(model.parameters(),lr=lr)\n",
    "scheduler=OneCycleLR(optimizer,lr,total_steps=epochs*len(train_dataloader),pct_start=0.25,anneal_strategy='cos')\n",
    "loss_fn=nn.MSELoss(reduction='mean')\n",
    "\n",
    "\n",
    "global_steps=0\n",
    "for i in range(epochs):\n",
    "    model.train()\n",
    "    for j, (image, target) in enumerate(train_dataloader):\n",
    "        noise=torch.randn_like(image).to(device)\n",
    "        image=image.to(device)\n",
    "        #### Fill in the training process here ####\n",
    "        #### calcualte the loss and optimize the model ####\n",
    "        \n",
    "        \n",
    "        \n",
    "        #### Fill in the training process here ####\n",
    "        scheduler.step()\n",
    "        if global_steps % model_ema_steps==0:\n",
    "            model_ema.update_parameters(model)\n",
    "        global_steps += 1\n",
    "        if j % log_frequency ==0:\n",
    "            print(\"Epoch[{}/{}],Step[{}/{}],loss:{:.5f},lr:{:.5f}\".format(i+1, epochs, j, len(train_dataloader),\n",
    "                                                                loss.detach().cpu().item(),scheduler.get_last_lr()[0]))\n",
    "    ckpt={\"model\":model.state_dict(),\n",
    "            \"model_ema\":model_ema.state_dict()}\n",
    "\n",
    "    os.makedirs(\"results\",exist_ok=True)\n",
    "    torch.save(ckpt,\"results/steps_{:0>8}.pt\".format(global_steps))\n",
    "\n",
    "    model_ema.eval()\n",
    "    samples=model_ema.module.sampling(n_samples, clipped_reverse_diffusion=True, device=device)\n",
    "    # You may need to customize this directory to fit your own device.\n",
    "    save_image(samples,\"results/steps_{:0>8}.png\".format(global_steps),nrow=int(math.sqrt(n_samples)))\n",
    "    \n",
    "    # Optional: You can visualize the generated samples in notebook by uncommenting the following code.\n",
    "    # grid = make_grid(samples, nrow=int(math.sqrt(n_samples)))\n",
    "    # plt.figure(figsize=(12, 6))\n",
    "    # plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "    # plt.axis('off')\n",
    "    # plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grouping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
