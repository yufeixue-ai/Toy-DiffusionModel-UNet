{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. We import basic packages and define fundamental utensils. The U-Net model is directly imported and you do not have to modify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "from diff_utils import Unet, ExponentialMovingAverage\n",
    "from tqdm import tqdm\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms \n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "def diffusion_mnist_dataloader(batch_size,image_size=28,num_workers=4):\n",
    "    \n",
    "    preprocess=transforms.Compose([transforms.Resize(image_size),\\\n",
    "                                    transforms.ToTensor(),\\\n",
    "                                    transforms.Normalize([0.5],[0.5])]) #[0,1] to [-1,1]\n",
    "\n",
    "    train_dataset=MNIST(root=\"../data\",\\\n",
    "                        train=True,\\\n",
    "                        download=True,\\\n",
    "                        transform=preprocess\n",
    "                        )\n",
    "    test_dataset=MNIST(root=\"../data\",\\\n",
    "                        train=False,\\\n",
    "                        download=True,\\\n",
    "                        transform=preprocess\n",
    "                        )\n",
    "\n",
    "    return DataLoader(train_dataset,batch_size=batch_size,shuffle=True,num_workers=num_workers),\\\n",
    "            DataLoader(test_dataset,batch_size=batch_size,shuffle=True,num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. We define the Diffusion toy model in this block. Please fill in the forward diffusion process and reverse diffusion process. Please observe the related varieties which may provide guidance and hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionToyModelforMNIST(nn.Module):\n",
    "    def __init__(self, image_size, in_channels, time_embedding_dim=256, timesteps=1000, base_dim=32, dim_mults= [1, 2, 4, 8]):\n",
    "        super().__init__()\n",
    "        self.timesteps=timesteps\n",
    "        self.in_channels=in_channels\n",
    "        self.image_size=image_size\n",
    "\n",
    "        betas=self._cosine_variance_schedule_generator(timesteps)\n",
    "\n",
    "        alphas=1.-betas\n",
    "        alphas_cumprod=torch.cumprod(alphas, dim=-1)\n",
    "\n",
    "        # It is an optional choice to use register_buffer. Other methods are applicable as well.\n",
    "        self.register_buffer(\"betas\", betas)\n",
    "        self.register_buffer(\"alphas\", alphas)\n",
    "        self.register_buffer(\"alphas_cumprod\", alphas_cumprod)\n",
    "        self.register_buffer(\"sqrt_alphas_cumprod\", torch.sqrt(alphas_cumprod))\n",
    "        self.register_buffer(\"sqrt_one_minus_alphas_cumprod\", torch.sqrt(1.-alphas_cumprod))\n",
    "\n",
    "        self.model=Unet(timesteps,time_embedding_dim,in_channels,in_channels,base_dim,dim_mults)\n",
    "\n",
    "    def forward(self,x,noise):\n",
    "        # x:NCHW\n",
    "        t=torch.randint(0,self.timesteps,(x.shape[0],)).to(x.device)\n",
    "        x_t=self._forward_diffusion(x,t,noise)\n",
    "        pred_noise=self.model(x_t,t)\n",
    "\n",
    "        return pred_noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sampling(self,n_samples,clipped_reverse_diffusion=True,device=\"cuda\"):\n",
    "        x_t=torch.randn((n_samples,self.in_channels,self.image_size,self.image_size)).to(device)\n",
    "        for i in tqdm(range(self.timesteps-1,-1,-1),desc=\"Sampling\"):\n",
    "            noise=torch.randn_like(x_t).to(device)\n",
    "            t=torch.tensor([i for _ in range(n_samples)]).to(device)\n",
    "\n",
    "            x_t=self._reverse_diffusion(x_t,t,noise)\n",
    "            assert torch.isnan(x_t).int().sum() == 0, \"nan in tensor.\"\n",
    "\n",
    "        x_t=(x_t+1.)/2. #[-1,1] to [0,1]\n",
    "\n",
    "        return x_t\n",
    "    \n",
    "    def _cosine_variance_schedule_generator(self, timesteps, epsilon= 0.008):\n",
    "        steps=torch.linspace(0,timesteps,steps=timesteps+1,dtype=torch.float32)\n",
    "        f_t=torch.cos(((steps/timesteps+epsilon)/(1.0+epsilon))*math.pi*0.5)**2\n",
    "        betas=torch.clip(1.0-f_t[1:]/f_t[:timesteps],0.0,0.999)\n",
    "\n",
    "        return betas\n",
    "\n",
    "    def _forward_diffusion(self,x_0,t,noise):\n",
    "        assert x_0.shape==noise.shape\n",
    "        #q(x_{t}|x_{t-1})\n",
    "        #### Fill in the forward process here ####\n",
    "        \n",
    "        # x_t = sqrt_alphas_cumprod * x_0 + sqrt_one_minus_alphas_cumprod * noise\n",
    "        sqrt_alphas_t_cumprod=self.sqrt_alphas_cumprod.gather(-1,t).reshape(x_0.shape[0],1,1,1)\n",
    "        sqrt_one_minus_alphas_t_cumprod=self.sqrt_one_minus_alphas_cumprod.gather(-1,t).reshape(x_0.shape[0],1,1,1)\n",
    "        x_t = sqrt_alphas_t_cumprod * x_0 + sqrt_one_minus_alphas_t_cumprod * noise\n",
    "        return x_t\n",
    "        \n",
    "        #### Fill in the forward process here ####\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _reverse_diffusion(self,x_t,t,noise): \n",
    "        '''\n",
    "        p(x_{0}|x_{t}),q(x_{t-1}|x_{0},x_{t})->mean,std\n",
    "\n",
    "        pred_noise -> pred_x_0 (clip to [-1.0,1.0]) -> pred_mean and pred_std\n",
    "        '''\n",
    "        pred=self.model(x_t,t)\n",
    "        alpha_t=self.alphas.gather(-1,t).reshape(x_t.shape[0],1,1,1)\n",
    "        alpha_t_cumprod=self.alphas_cumprod.gather(-1,t).reshape(x_t.shape[0],1,1,1)\n",
    "        beta_t=self.betas.gather(-1,t).reshape(x_t.shape[0],1,1,1)\n",
    "        #### Fill in the backward process here ####\n",
    "        alpha_t_1_cumprod=self.alphas_cumprod.gather(-1,t-1).reshape(x_t.shape[0],1,1,1)\n",
    "        var = ( (1.-alpha_t_1_cumprod) / (1.-alpha_t_cumprod) ) * beta_t\n",
    "        \n",
    "        sqrt_alphas_t=torch.sqrt(alpha_t)\n",
    "        sqrt_one_minus_alphas_t_cumprod=self.sqrt_one_minus_alphas_cumprod.gather(-1,t).reshape(x_t.shape[0],1,1,1)\n",
    "        mean = 1/sqrt_alphas_t * (x_t - ((1.-alpha_t) / (sqrt_one_minus_alphas_t_cumprod)) * pred)\n",
    "        #### Fill in the backward process here ####\n",
    "\n",
    "        if t.min()>0:\n",
    "            #### Fill in the backward process here ####\n",
    "            std=torch.sqrt(var)           \n",
    "            #### Fill in the backward process here ####\n",
    "        else:\n",
    "            #### Fill in the backward process here ####\n",
    "\n",
    "            #### Fill in the backward process here ####\n",
    "            std=0.0\n",
    "\n",
    "        return mean+std*noise \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define all of the hyperparameters. You can customize them based on your own training environment, if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 128   # If default 128 batch size is too large for your GPU, try reducing it.\n",
    "model_ema_steps = 10\n",
    "model_ema_decay = 0.995  # These two parameters are used for the Exponential Moving Average (EMA) of the model. \n",
    "log_frequency = 50  # The frequency for printing the log message during training.\n",
    "lr = 0.001  # Learning rate.\n",
    "timesteps = 1000      # Sampling steps of the diffusion DDPM process.\n",
    "\n",
    "# We do not recommand you to change below hyperparameters, maintaining a fair training comparison and unified visualization.\n",
    "epochs = 100\n",
    "n_samples = 36\n",
    "model_base_dim = 64   # The base dimension of the UNet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. This is the main training process of our toy diffusion model. Please fill in the blank to complete the loss calulation and optimization steps. We provide multiple ways for you to observe your training progress for your customization. Try and play!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice:\u001b[39m\u001b[38;5;124m\"\u001b[39m,device)\n\u001b[1;32m      2\u001b[0m train_dataloader,test_dataloader\u001b[38;5;241m=\u001b[39mdiffusion_mnist_dataloader(batch_size\u001b[38;5;241m=\u001b[39mbatch_size,image_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m28\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m model\u001b[38;5;241m=\u001b[39m\u001b[43mDiffusionToyModelforMNIST\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_base_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdim_mults\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#torchvision ema setting\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#https://github.com/pytorch/vision/blob/main/references/classification/train.py#L317\u001b[39;00m\n\u001b[1;32m     11\u001b[0m adjust \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m batch_size \u001b[38;5;241m*\u001b[39m model_ema_steps \u001b[38;5;241m/\u001b[39m epochs\n",
      "File \u001b[0;32m~/anaconda3/envs/course/lib/python3.9/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/course/lib/python3.9/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/course/lib/python3.9/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 802 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/course/lib/python3.9/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/course/lib/python3.9/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/course/lib/python3.9/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"device:\",device)\n",
    "train_dataloader,test_dataloader=diffusion_mnist_dataloader(batch_size=batch_size,image_size=28)\n",
    "model=DiffusionToyModelforMNIST(timesteps=timesteps,\n",
    "            image_size=28,\n",
    "            in_channels=1,\n",
    "            base_dim=model_base_dim,\n",
    "            dim_mults=[2,4]).to(device)\n",
    "\n",
    "#torchvision ema setting\n",
    "#https://github.com/pytorch/vision/blob/main/references/classification/train.py#L317\n",
    "adjust = 1 * batch_size * model_ema_steps / epochs\n",
    "alpha = 1.0 - model_ema_decay\n",
    "alpha = min(1.0, alpha * adjust)\n",
    "model_ema = ExponentialMovingAverage(model, device=device, decay=1.0 - alpha)\n",
    "\n",
    "optimizer=AdamW(model.parameters(),lr=lr)\n",
    "scheduler=OneCycleLR(optimizer,lr,total_steps=epochs*len(train_dataloader),pct_start=0.25,anneal_strategy='cos')\n",
    "loss_fn=nn.MSELoss(reduction='mean')\n",
    "\n",
    "\n",
    "global_steps=0\n",
    "for i in range(epochs):\n",
    "    model.train()\n",
    "    for j, (image, target) in enumerate(train_dataloader):\n",
    "        noise=torch.randn_like(image).to(device)\n",
    "        image=image.to(device)\n",
    "        #### Fill in the training process here ####\n",
    "        #### calcualte the loss and optimize the model ####\n",
    "        out = model(image, noise)\n",
    "        loss = loss_fn(out, noise)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()       \n",
    "        \n",
    "        #### Fill in the training process here ####\n",
    "        scheduler.step()\n",
    "        if global_steps % model_ema_steps==0:\n",
    "            model_ema.update_parameters(model)\n",
    "        global_steps += 1\n",
    "        if j % log_frequency ==0:\n",
    "            print(\"Epoch[{}/{}],Step[{}/{}],loss:{:.5f},lr:{:.5f}\".format(i+1, epochs, j, len(train_dataloader),\n",
    "                                                                loss.detach().cpu().item(),scheduler.get_last_lr()[0]))\n",
    "    ckpt={\"model\":model.state_dict(),\n",
    "            \"model_ema\":model_ema.state_dict()}\n",
    "\n",
    "    os.makedirs(\"results\",exist_ok=True)\n",
    "    torch.save(ckpt,\"results/steps_{:0>8}.pt\".format(global_steps))\n",
    "\n",
    "    model_ema.eval()\n",
    "    samples=model_ema.module.sampling(n_samples, clipped_reverse_diffusion=True, device=device)\n",
    "    # You may need to customize this directory to fit your own device.\n",
    "    save_image(samples,\"results/steps_{:0>8}.png\".format(global_steps),nrow=int(math.sqrt(n_samples)))\n",
    "    \n",
    "    # Optional: You can visualize the generated samples in notebook by uncommenting the following code.\n",
    "    # grid = make_grid(samples, nrow=int(math.sqrt(n_samples)))\n",
    "    # plt.figure(figsize=(12, 6))\n",
    "    # plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "    # plt.axis('off')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grouping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
